{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import Tensor, IntTensor\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphabet = [' ','a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v' ,'w', 'x', 'y', 'z', \n",
    " '0', '1', '2','3', '4', '5', '6', '7','8','9','-', ';', '.', '!', '?', ':', '’', '\\\\', '|', '_', '@', '#', '$', '%', 'ˆ', '&', '*', \n",
    " '˜', '‘', '+', '-', '=', '<', '>','(', ')', '[',']', '{', '}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "indexing = { letter : i+1 for i, letter in enumerate(alphabet)}\n",
    "indexing['UNK'] = len(alphabet)\n",
    "indexing['No_letter'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataGenerator(train_split=0.8,binary=False, max_length=1014): \n",
    "    \n",
    "    dataset = []\n",
    "\n",
    "    cwd = os.getcwd()\n",
    "    for filename in ['pos', 'neg']:\n",
    "        file_dir = join(cwd, 'aclImdb', 'train', filename)\n",
    "        files = [f for f in listdir(file_dir) if isfile(join(file_dir, f))]\n",
    "        for f in files: \n",
    "            name = f.split('.')[0]\n",
    "            name = name.split('_')\n",
    "            id = name[0]\n",
    "            rating = name[1]\n",
    "            if(binary): \n",
    "                if(filename=='pos'): \n",
    "                    rating=1\n",
    "                else: \n",
    "                    rating=0\n",
    "            \n",
    "            path = join(file_dir, f)\n",
    "            review = torch.zeros(max_length).long()\n",
    "            with open(path) as myfile:\n",
    "                data=myfile.read()\n",
    "                for i in range(min(max_length,len(data))):\n",
    "                    letter = data[i].lower()\n",
    "                    if letter in alphabet:\n",
    "                        review[i] = indexing[letter]\n",
    "                    else:\n",
    "                        review[i] = indexing['UNK']\n",
    "\n",
    "                dataset.append({'review': review, 'rating': torch.IntTensor([int(rating) -  1])})\n",
    "    \n",
    "    #random split 0.8 / 0.2\n",
    "    dataset_train, dataset_val =  train_test_split(dataset, test_size=1-train_split)\n",
    "    \n",
    "    return dataset_train, dataset_val\n",
    "\n",
    "def data_iter(dataset, batch_size=32):   \n",
    "    dataset_size = len(dataset)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while True:\n",
    "        start += batch_size\n",
    "        if start > dataset_size - batch_size:\n",
    "            # Start another epoch.\n",
    "            start = 0\n",
    "            random.shuffle(order)   \n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        yield [dataset[index] for index in batch_indices]\n",
    "\n",
    "# This is the iterator we use when we're evaluating our model. \n",
    "# It gives a list of batches that you can then iterate through.\n",
    "def eval_iter(source, batch_size):\n",
    "    batches = []\n",
    "    dataset_size = len(source)\n",
    "    start = -1 * batch_size\n",
    "    order = list(range(dataset_size))\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch = [source[index] for index in batch_indices]\n",
    "        if len(batch) == batch_size:\n",
    "            batches.append(batch)\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "    return batches\n",
    "\n",
    "# The following function gives batches of vectors and labels, \n",
    "# these are the inputs to your model and loss function\n",
    "def get_batch(batch):\n",
    "    vectors = []\n",
    "    labels = []\n",
    "    for dict in batch:\n",
    "        vectors.append(dict[\"review\"])\n",
    "        labels.append(dict[\"rating\"])\n",
    "    return vectors, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Char_CNN_Small(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 fully_layers,\n",
    "                 l0,\n",
    "                 alphabet_size,\n",
    "                 nb_classes,\n",
    "                 batch_size,\n",
    "                 ):\n",
    "        super(Char_CNN_Small,self).__init__()\n",
    "        \n",
    "        self.conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    [256, 7, 3],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                    ]\n",
    "        self.fully_layers = fully_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.l0 = l0\n",
    "        \n",
    "        self.convs = []\n",
    "        self.linear = []\n",
    "        self.max_pool = nn.MaxPool1d(3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        in_feat = alphabet_size\n",
    "        for out_feat, kernel_size, max_pool in conv_layers:\n",
    "            conv = nn.Conv1d(in_feat, out_feat, kernel_size)\n",
    "            print(conv.weight.size())\n",
    "            self.convs.append(conv)\n",
    "            in_feat = out_feat\n",
    "        \n",
    "        l6 = int((l0 - 96)/27)\n",
    "        in_feat = l6*out_feat\n",
    "        \n",
    "        for out_feat in fully_layers:\n",
    "            self.linear.append(nn.Linear(in_feat, out_feat))\n",
    "            in_feat = out_feat\n",
    "        \n",
    "        self.classifier = nn.Linear(in_feat, nb_classes)\n",
    "        \n",
    "        if self.nb_classes == 2:\n",
    "            self.class_non_lin = nn.Sigmoid()\n",
    "        else:\n",
    "            self.class_non_lin = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for conv in self.convs[:2]:\n",
    "            out = conv(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.max_pool(out)\n",
    "        \n",
    "        for conv in self.convs[2:5]:\n",
    "            out = conv(out)\n",
    "            out = self.relu(out)\n",
    "            \n",
    "            \n",
    "        out = self.convs[5](out)\n",
    "        out = self.relu(out)\n",
    "        out = self.max_pool(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        \n",
    "        \n",
    "        for lin in self.linear:\n",
    "            out = lin(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "        \n",
    "#        print(out.data.numpy().shape)\n",
    "#        print(out.data.numpy()[:, :10])\n",
    "        out = self.classifier(out)\n",
    "        out = self.class_non_lin(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for conv in self.convs:\n",
    "            nn.init.normal(conv.weight, mean=0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        \n",
    "        vectors = torch.stack(vectors)\n",
    "        vectors_ = torch.unsqueeze(vectors, 1)\n",
    "        one_hot = torch.FloatTensor(batch_size, alphabet_size, l0).zero_()\n",
    "        one_hot.scatter_(1, vectors_, 1)\n",
    "        vectors = Variable(one_hot)\n",
    "\n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "        \n",
    "        output = model(vectors)\n",
    "#        print(vectors.data.numpy().sum(axis=2))\n",
    "#        print(output)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        \n",
    "        total += len(labels)\n",
    "        correct += np.equal(predicted.data.numpy(), labels.data.numpy()).sum()\n",
    "      \n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, validation_iter, train_eval_iter):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "    #total_batches = int(len(training_set) / batch_size)\n",
    "    total_batches = int(200 / batch_size)\n",
    "    \n",
    "    print(\"total_bat\", total_batches)\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        vectors, labels = get_batch(next(training_iter)) \n",
    "\n",
    "        vectors = torch.stack(vectors)\n",
    "        vectors_ = torch.unsqueeze(vectors, 1)\n",
    "        one_hot = torch.FloatTensor(batch_size, alphabet_size, l0).zero_()\n",
    "        one_hot.scatter_(1, vectors_, 1)\n",
    "        \n",
    "        vectors = Variable(one_hot) # batch_size, seq_len\n",
    "        \n",
    "        labels = Variable(torch.stack(labels).squeeze())\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        output = model(vectors)\n",
    "        lossy = loss_(output, labels.long())\n",
    "        lossy.backward()\n",
    "#        torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)\n",
    "        optim.step()\n",
    "        \n",
    "        if step % total_batches == 0:\n",
    "            if epoch % 5 == 0:\n",
    "                print(\"begin print\")\n",
    "                print(\"Epoch %i; Step %i; Loss %f; Train acc: %f; Dev acc %f\" \n",
    "                      %(epoch, step, lossy.data[0],\\\n",
    "                        evaluate(model, train_eval_iter),\\\n",
    "                        0))#evaluate(model, validation_iter)))\n",
    "            epoch += 1\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## generate dataset\n",
    "\n",
    "training_set, validation_set = dataGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 68, 7])\n",
      "torch.Size([256, 256, 7])\n",
      "torch.Size([256, 256, 3])\n",
      "torch.Size([256, 256, 3])\n",
      "torch.Size([256, 256, 3])\n",
      "torch.Size([256, 256, 3])\n",
      "total_bat 25\n",
      "8112\n",
      "begin print\n",
      "Epoch 0; Step 0; Loss 2.302569; Train acc: 0.125000; Dev acc 0.000000\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n",
      "8112\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-37b518a9b480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mvalidation_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-c263253bde19>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(batch_size, num_epochs, model, loss_, optim, training_iter, validation_iter, train_eval_iter)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mlossy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlossy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/etienne/miniconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-19372e2e6b41>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/etienne/miniconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/etienne/miniconda2/envs/py36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 154\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/etienne/miniconda2/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv1d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     81\u001b[0m     f = ConvNd(_single(stride), _single(padding), _single(dilation), False,\n\u001b[1;32m     82\u001b[0m                _single(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyper Parameters \n",
    "conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    [256, 7, 3],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                    ]\n",
    "\n",
    "fully_layers = [1024, 1024]\n",
    "l0 = 1014\n",
    "alphabet_size = 68\n",
    "nb_classes = 10\n",
    "batch_size = 8\n",
    "\n",
    "learning_rate = 0.008\n",
    "num_epochs = 20\n",
    "\n",
    "\n",
    "# Build, initialize model\n",
    "model = Char_CNN_Small(fully_layers, l0, alphabet_size, nb_classes, batch_size)\n",
    "model.init_weights()\n",
    "\n",
    "# Loss and Optimizer\n",
    "\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = data_iter(training_set[:8], batch_size)\n",
    "train_eval_iter = eval_iter(training_set[:8], batch_size)\n",
    "validation_iter = eval_iter(validation_set[:8], batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, model, loss, optimizer, training_iter, validation_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(training_set[0]['review'].numpy() == 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
