{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(s,max_length=1014): \n",
    "    A = []\n",
    "    m = min(max_length,len(s))\n",
    "    for i in range(m):\n",
    "        c = s[i]\n",
    "        A.append(ord(c))\n",
    "    return np.array(A)\n",
    "        \n",
    "def dataGenerator(train_split=0.8,binary=False): \n",
    "    reviews = []\n",
    "    ratings = []\n",
    "    \n",
    "    for filename in ['pos', 'neg']:\n",
    "        cwd=os.getcwd()\n",
    "        print(cwd)\n",
    "        files = [f for f in listdir(join(cwd, 'train', filename)) if isfile(join(filename, f))]\n",
    "        print(len(files))\n",
    "        for f in files: \n",
    "            name = f.split('.')[0]\n",
    "            name = name.split('_')\n",
    "            id = name[0]\n",
    "            rating = name[1]\n",
    "            if(binary): \n",
    "                if(filename=='pos'): \n",
    "                    rating=1\n",
    "                else: \n",
    "                    rating=0\n",
    "            ratings.append(rating)\n",
    "            path = filename+'/'+f\n",
    "            with open(path) as myfile:\n",
    "                data=myfile.read()\n",
    "                data=tokenize(data)\n",
    "                reviews.append(data)\n",
    "                \n",
    "    reviews = np.array(reviews)\n",
    "    ratings = np.array(ratings,dtype=float)\n",
    "    #rewiews, ratings\n",
    "    \n",
    "    #random split 0.8 / 0.2\n",
    "    reviews_train, reviews_val, ratings_train, ratings_val =  train_test_split(reviews, ratings, test_size=1-train_split)\n",
    "    return reviews_train, ratings_train, reviews_val, ratings_val\n",
    "\n",
    "def batchify(review, rating, batch_size=32):   \n",
    "    batches1 = []\n",
    "    batches2 = []\n",
    "    dataset_size = len(review)\n",
    "    start = -1 * batch_size\n",
    "    order = np.arange(dataset_size)\n",
    "    random.shuffle(order)\n",
    "\n",
    "    while start < dataset_size - batch_size:\n",
    "        start += batch_size\n",
    "        batch_indices = order[start:start + batch_size]\n",
    "        batch1 = [review[index] for index in batch_indices]\n",
    "        batch2 = [rating[index] for index in batch_indices]\n",
    "        batches1.append(batch1)\n",
    "        batches2.append(batch2)\n",
    "        \n",
    "    return np.array(batches1), np.array(batches2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saad/Downloads/NLP/NLP_project/characterLevelCNN\n",
      "0\n",
      "/Users/saad/Downloads/NLP/NLP_project/characterLevelCNN\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "reviews_train, ratings_train, reviews_val, ratings_val = dataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Char_CNN_Small(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 fully_layers,\n",
    "                 l0,\n",
    "                 alphabet_size,\n",
    "                 nb_classes,\n",
    "                 batch_size,\n",
    "                 ):\n",
    "        super(Char_CNN_Small,self).__init__()\n",
    "        \n",
    "        self.conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    [256, 7, 3],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                    ]\n",
    "        self.fully_layers = fully_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_classes = nb_classes\n",
    "        \n",
    "        self.convs = []\n",
    "        self.linear = []\n",
    "        self.max_pool = nn.MaxPool1d(3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        in_feat = alphabet_size\n",
    "        for out_feat, kernel_size, max_pool in conv_layers:\n",
    "            conv = nn.Conv1d(in_feat, out_feat, kernel_size)\n",
    "            self.convs.append(conv)\n",
    "            in_feat = out_feat\n",
    "        \n",
    "        l6 = int((l0 - 96)/27)\n",
    "        in_feat = l6*out_feat\n",
    "        \n",
    "        for out_feat in fully_layers:\n",
    "            self.linear.append(nn.Linear(in_feat, out_feat))\n",
    "            in_feat = out_feat\n",
    "        \n",
    "        self.classifier = nn.Linear(in_feat, nb_classes)\n",
    "        \n",
    "        if self.nb_classes == 2:\n",
    "            self.class_non_lin = nn.Sigmoid()\n",
    "        else:\n",
    "            self.class_non_lin = nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x \n",
    "        print(out.size())\n",
    "        for conv in self.convs[:2]:\n",
    "            out = conv(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.max_pool(out)\n",
    "            print(out.size())\n",
    "            \n",
    "        for conv in self.convs[2:5]:\n",
    "            out = conv(out)\n",
    "            out = self.relu(out)\n",
    "            print(out.size())\n",
    "            \n",
    "            \n",
    "        out = self.convs[5](out)\n",
    "        out = self.relu(out)\n",
    "        out = self.max_pool(out)\n",
    "        print(out.size())\n",
    "        \n",
    "        out = out.view(batch_size, -1)\n",
    "        print(out.size())\n",
    "        \n",
    "        for lin in self.linear:\n",
    "            out = lin(out)\n",
    "            out = self.relu(out)\n",
    "            out = self.dropout(out)\n",
    "            print(out.size())\n",
    "\n",
    "            \n",
    "        out = self.classifier(out)\n",
    "        out = self.class_non_lin(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def init_weights(self):\n",
    "        for conv in self.convs:\n",
    "            nn.init.normal(conv.weight, mean=0, std=0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(data_iter)):\n",
    "        vectors, labels = get_batch(data_iter[i])\n",
    "        #vectors = Variable(torch.stack(vectors).squeeze())\n",
    "        #labels = torch.stack(labels).squeeze()\n",
    "        \n",
    "        output, hidden = model(vectors, hidden)\n",
    "        \n",
    "        predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum()\n",
    "      \n",
    "    return correct / float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_loop(batch_size, num_epochs, model, loss_, optim, training_iter, eval_iter, train_eval_iter):\n",
    "    step = 0\n",
    "    epoch = 0\n",
    "\n",
    "    while epoch <= num_epochs:\n",
    "        model.train()\n",
    "        for vectors, labels in training_iter:\n",
    "            vectors, labels = next(training_iter) \n",
    "           # vectors = Variable(torch.stack(vectors).squeeze()) # batch_size, seq_len\n",
    "           # labels = Variable(torch.stack(labels).squeeze())\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "            output = model(vectors, hidden, c_t)\n",
    "\n",
    "            lossy = loss_(output, labels)\n",
    "            lossy.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), 5.0)\n",
    "            optim.step()\n",
    "        \n",
    "        if epoch % 1 == 0:\n",
    "            print(\"Epoch %i; Step %i; Loss %f; Train acc: %f; Eval acc %f\" \n",
    "                  %(epoch, step, lossy.data[0],\\\n",
    "                    evaluate(model, train_eval_iter, lstm),\\\n",
    "                    evaluate(model, eval_iter, lstm)))\n",
    "        epoch += 1\n",
    "        next(training_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loading \n",
    "reviews_train, ratings_train, reviews_val, ratings_val = dataGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-13f8660321e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0meval_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mratings_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_eval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-06a503d3a82a>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(batch_size, num_epochs, model, loss_, optim, training_iter, eval_iter, train_eval_iter)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m            \u001b[0;31m# vectors = Variable(torch.stack(vectors).squeeze()) # batch_size, seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "# Hyper Parameters \n",
    "conv_layers = [\n",
    "                    [256, 7, 3],\n",
    "                    [256, 7, 3],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, None],\n",
    "                    [256, 3, 3]\n",
    "                    ]\n",
    "\n",
    "fully_layers = [1024, 1024]\n",
    "l0 = 1014\n",
    "alphabet_size = 69\n",
    "nb_classes = 4\n",
    "batch_size = 256\n",
    "\n",
    "learning_rate = 0.2\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "# Build, initialize model\n",
    "model = Char_CNN_Small(fully_layers, l0, alphabet_size, nb_classes, batch_size)\n",
    "model.init_weights()\n",
    "\n",
    "# Loss and Optimizer\n",
    "loss = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "training_iter = batchify(reviews_train, ratings_train, batch_size)\n",
    "train_eval_iter = batchify(reviews_train[:500], ratings_train[:500],batch_size)\n",
    "eval_iter = batchify(reviews_val, ratings_val, batch_size)\n",
    "\n",
    "training_loop(batch_size, num_epochs, model, loss, optimizer, training_iter, eval_iter, train_eval_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_batches = int(len(training_iter) / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4fded1327660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mvectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "for vectors, labels in training_iter:\n",
    "    print(vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104448"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "102*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
